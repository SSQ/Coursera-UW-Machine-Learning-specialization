# Machine Learning Specialization

Course can be found in [Coursera](https://www.coursera.org/specializations/machine-learning)

Partial notes can be found in my blog [SSQ](https://ssq.github.io/2017/08/19/Coursera%20UW%20Machine%20Learning%20Specialization%20Notebook/)

## 1. Machine Learning Foundations: A Case Study Approach
Course can be found in [Coursera](https://www.coursera.org/learn/ml-foundations)
<table>
  <tr>
    <th>Programming Assignments</th>
  </tr>
  <tr>
    <td>
      <ul>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Programming%20Assignment%201">Familiar with Ipython notebook and Sframe</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Programming%20Assignment%202">Implement Linear Regression model with different several features</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Programming%20Assignment%203">Implement Logistic Regression model with different several features</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Week%204%20PA%201">Retrieving Wikipedia articles</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Week%205%20PA%201">Recommending songs</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Week%206%20PA%201">Deep Features for Image Classification</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Week%206%20PA%202">Deep Features for Image Retrieval</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Week%206%20PA%203">Deep Features for Image Retrieval Answers</a>
        </li>
      </ul>
    </td>
  </tr>
</table>

Slides and more details about this course can be found in my Github [SSQ](https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach)
  
- Week 1 Introduction
  - Regression. Case study: Predicting house prices
  - Classification. Case study: Analyzing sentiment
  - Clustering & Retrieval. Case study: Finding documents
  - Matrix Factorization & Dimensionality Reduction. Case study: Recommending Products
  - Capstone. An intelligent application using deep learning
  - [x] [Familiar with Ipython notebook and Sframe](https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Programming%20Assignment%201)
- Week 2 Regression Predicting House Prices
  - Linear Regression
  - Adding higher order effects
  - Evaluating overfitting via training/test split
  - Adding other features
  - Other regression examples
  - [x] [Implement Linear Regression model with different several features](https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Programming%20Assignment%202)

- Week 3 Classification Analyzing Sentiment
  - Classifier applications
  - Linear classifiers
  - Decision boundaries
  - Training and evaluating a classifier
  - What’s a good accuracy?
  - False positives, false negatives, and confusion matrices
  - Learning curves: How much data do I need?
  - Class probabilities
  - [x] [Implement Logistic Regression model with different several features](https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Programming%20Assignment%203)

## 2. Machine Learning: Regression
Course can be found in [Coursera](https://www.coursera.org/learn/ml-regression)
<table>
  <tr>
    <th>Description</th>
    <th>Programming Assignments</th>
  </tr>
  <tr>
    <td>
      <table>
        <tr>
          <td>Models</td>
          <td><ul><li>Linear regression</li><li>Regularization: Ridge (L2), Lasso (L1)</li><li>Nearest neighbor and kernel regression</li></ul></td>
        </tr>
        <tr>
          <td>Algorithms</td>
          <td><ul><li>Gradient descent</li><li>Coordinate descent</li></ul></td>
        </tr>
        <tr>
          <td>Concepts</td>
          <td><ul><li>Loss functions, bias-variance tradeoff</li><li>cross-validation, sparsity, overfitting</li><li>model selection, feature selection</li></ul></td>
        </tr>
      </table>
    </td>
    <td>
      <ul>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%201">Fitting a simple linear regression model on housing data</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%202">Exploring different multiple regression models for house price prediction</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%203">Implementing gradient descent for multiple regression</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%204">Exploring the bias-variance tradeoff</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%205">Observing effects of L2 penalty in polynomial regression</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%206">Implementing ridge regression via gradient descent</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%207">Using LASSO to select features</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%208">Implementing LASSO using coordinate descent</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Week%206%20PA%201">Predicting house prices using k-nearest neighbors regression</a>
        </li>
      </ul>
    </td>
  </tr>
</table>
  
Slides and more details about this course can be found in my Github [SSQ](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression)

- Week 1: Simple Linear Regression:
    - Describe the input (features) and output (real-valued predictions) of a regression model
    - Calculate a goodness-of-fit metric (e.g., RSS)
    - Estimate model parameters to minimize RSS using gradient descent
    - Interpret estimated model parameters
    - Exploit the estimated model to form predictions
    - Discuss the possible influence of high leverage points
    - Describe intuitively how fitted line might change when assuming different goodness-of-fit metrics
    - [x] [Fitting a simple linear regression model on housing data](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%201) 
    
- Week 2: Multiple Regression: Linear regression with multiple features
    - Describe polynomial regression
    - Detrend a time series using trend and seasonal components
    - Write a regression model using multiple inputs or features thereof
    - Cast both polynomial regression and regression with multiple inputs as regression with multiple features
    - Calculate a goodness-of-fit metric (e.g., RSS)
    - Estimate model parameters of a general multiple regression model to minimize RSS:
      - In closed form
      - Using an iterative gradient descent algorithm
    - Interpret the coefficients of a non-featurized multiple regression fit
    - Exploit the estimated model to form predictions
    - Explain applications of multiple regression beyond house price modeling
    - [x] [Exploring different multiple regression models for house price prediction](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%202)
    - [x] [Implementing gradient descent for multiple regression](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%203)
    
- Week 3: Assessing Performance
    - Describe what a loss function is and give examples
    - Contrast training, generalization, and test error
    - Compute training and test error given a loss function
    - Discuss issue of assessing performance on training set
    - Describe tradeoffs in forming training/test splits
    - List and interpret the 3 sources of avg. prediction error
      - Irreducible error, bias, and variance
    - Discuss issue of selecting model complexity on test data and then using test error to assess generalization error
    - Motivate use of a validation set for selecting tuning parameters (e.g., model complexity)
    - Describe overall regression workflow
    - [x] [Exploring the bias-variance tradeoff](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%204)
    
- Week 4: Ridge Regression
    - Describe what happens to magnitude of estimated coefficients when model is overfit
    - Motivate form of ridge regression cost function
    - Describe what happens to estimated coefficients of ridge regression as tuning parameter λ is varied
    - Interpret coefficient path plot
    - Estimate ridge regression parameters:
      - In closed form
      - Using an iterative gradient descent algorithm
    - Implement K-fold cross validation to select the ridge regression tuning parameter λ
    - [x] [Observing effects of L2 penalty in polynomial regression](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%205)
    - [x] [Implementing ridge regression via gradient descent](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%206)
    
- Week 5: Lasso Regression: Regularization for feature selection
    - Perform feature selection using “all subsets” and “forward stepwise” algorithms
    - Analyze computational costs of these algorithms
    - Contrast greedy and optimal algorithms
    - Formulate lasso objective
    - Describe what happens to estimated lasso coefficients as tuning parameter λ is varied
    - Interpret lasso coefficient path plot
    - Contrast ridge and lasso regression
    - Describe geometrically why L1 penalty leads to sparsity
    - Estimate lasso regression parameters using an iterative coordinate descent algorithm
    - Implement K-fold cross validation to select lasso tuning parameter λ
    - [x] [Using LASSO to select features](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%207)
    - [x] [Implementing LASSO using coordinate descent](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Programming%20Assignment%208)
    
- Week 6: Going nonparametric: Nearest neighbor and kernel regression
  - Motivate the use of nearest neighbor (NN) regression
  - Define distance metrics in 1D and multiple dimensions
  - Perform NN and k-NN regression
  - Analyze computational costs of these algorithms
  - Discuss sensitivity of NN to lack of data, dimensionality, and noise
  - Perform weighted k-NN and define weights using a kernel
  - Define and implement kernel regression
  - Describe the effect of varying the kernel bandwidth λ or # of nearest neighbors k
  - Select λ or k using cross validation
  - Compare and contrast kernel regression with a global average fit
  - Define what makes an approach nonparametric and why NN and kernel regression are considered nonparametric methods
  - Analyze the limiting behavior of NN regression
  - Use NN for classification
  - [x] [Predicting house prices using k-nearest neighbors regression](https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/tree/master/Week%206%20PA%201)

## 3. Machine Learning: Classification
Course can be found in [Coursera](https://www.coursera.org/learn/ml-classification)

<table>
  <tr>
    <th>Description</th>
    <th>Programming Assignments</th>
  </tr>
  <tr>
    <td>
      <table>
        <tr>
          <td>Models</td>
          <td><ul><li>Linear classifiers</li><li>Logistic regression</li><li>Decision trees</li><li>Ensembles</li></ul></td>
        </tr>
        <tr>
          <td>Algorithms</td>
          <td><ul><li>Stochastic gradient descent</li><li>Recursive greedy</li><li>Boosting</li></ul></td>
        </tr>
        <tr>
          <td>Concepts</td>
          <td><ul><li>Decision boundaries, MLE</li><li>ensemble methods, online learning</li></ul></td>
        </tr>
        <tr>
          <td>Core ML</td>
          <td><ul><li>Alleviating overfitting</li><li>Handling missing data</li><li>Precision-recall</li><li>Online learning</li></ul></td>
        </tr>
      </table>
    </td>
    <td>
      <ul>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%201">Predicting sentiment from product reviews</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%202">Implementing logistic regression from scratch</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%203">Implementing Logistic Regression with L2 regularization</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%204">Identifying safe loans with decision trees</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%205">Implementing binary decision trees from scratch</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%206">Decision Trees in Practice for preventing overfitting</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Week%205%20PA%201">Exploring Ensemble Methods with pre-implemented gradient boosted trees</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Week%205%20PA%202">Implement your own boosting module</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Week%206%20PA%201">Exploring precision and recall</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Week%207%20PA%201">Training Logistic Regression via Stochastic Gradient Ascent</a>
        </li>
      </ul>
    </td>
  </tr>
</table>
    
Slides and more details about this course can be found in my [Github](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification)

- Week 1: 
  - Linear Classifiers & Logistic Regression
    - decision boundaries
    - linear classifiers
    - class probability
    - logistic regression
    - impact of coefficient values on logistic regression output
    - 1-hot encoding
    - multiclass classification using the 1-versus-all
    - [x] [Predicting sentiment from product reviews](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%201)
- Week 2: 
  - Learning Linear Classifiers
    - Maximum likelihood estimation
    - Gradient ascent algorithm for learning logistic regression classifier
    - Choosing step size for gradient ascent/descent
    - (VERY OPTIONAL LESSON) Deriving gradient of logistic regression
    - [x] [Implementing logistic regression from scratch](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%202)  
  - Overfitting & Regularization in Logistic Regression
    - Overfitting in classification
    - Overconfident predictions due to overfitting
    - L2 regularized logistic regression
    - Sparse logistic regression
    - [x] [Implementing Logistic Regression with L2 regularization](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%203)
- Week 3:
  - Decision Trees
    - Predicting loan defaults with decision trees
    - Learning decision trees
        - Recursive greedy algorithm
        - Learning a decision stump
        - Selecting best feature to split on
        - When to stop recursing
    - Using the learned decision tree
        - Traverse a decision tree to make predictions: Majority class predictions; Probability predictions; Multiclass classification
    - Learning decision trees with continuous inputs
        - Threshold splits for continuous inputs
        - (OPTIONAL) Picking the best threshold to split on
    - [x] [Identifying safe loans with decision trees](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%204)
    - [x] [Implementing binary decision trees from scratch](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%205)
- Week 4
  - Overfitting in decision trees 
    - Identify when overfitting in decision trees
    - Prevent overfitting with early stopping
      - Limit tree depth
      - Do not consider splits that do not reduce classification error
      - Do not split intermediate nodes with only few points
    - Prevent overfitting by pruning complex trees
      - Use a total cost formula that balances classification error and tree complexity
      - Use total cost to merge potentially complex trees into simpler ones 
    - [x] [Decision Trees in Practice for preventing overfitting](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Programming%20Assignment%206) 
  - Handling missing data 
    - Describe common ways to handling missing data:
      1. Skip all rows with any missing values
      2. Skip features with many missing values
      3. Impute missing values using other data points
    - Modify learning algorithm (decision trees) to handle missing data:
      1. Missing values get added to one branch of split
      2. Use classification error to determine where missing values go 
- Week 5
  - Boosting 
    - Identify notion ensemble classifiers
    - Formalize ensembles as the weighted combination of simpler classifiers
    - Outline the boosting framework – sequentially learn classifiers on weighted data
    - Describe the AdaBoost algorithm
      - Learn each classifier on weighted data
      - Compute coefficient of classifier
      - Recompute data weights
      - Normalize weights
    - Implement AdaBoost to create an ensemble of decision stumps
    - Discuss convergence properties of AdaBoost & how to pick the maximum number of iterations T 
    - [x] [Exploring Ensemble Methods with pre-implemented gradient boosted trees](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Week%205%20PA%201)
    - [x] [Implement your own boosting module](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Week%205%20PA%202)
- Week 6 
  - Evaluating classifiers: Precision & Recall 
    - Classification accuracy/error are not always right metrics
    - Precision captures fraction of positive predictions that are correct
    - Recall captures fraction of positive data correctly identified by the model
    - Trade-off precision & recall by setting probability thresholds
    - Plot precision-recall curves.
    - Compare models by computing precision at k
    - [x] [Exploring precision and recall](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Week%206%20PA%201)
- Week 7
  - Scaling to Huge Datasets & Online Learning 
    - Significantly speedup learning algorithm using stochastic gradient
    - Describe intuition behind why stochastic gradient works
    - Apply stochastic gradient in practice
    - Describe online learning problems
    - Relate stochastic gradient to online learning 
    - [x] [Training Logistic Regression via Stochastic Gradient Ascent](https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Week%207%20PA%201)

## 4. Machine Learning: Clustering & Retrieval
Course can be found in [Coursera](https://www.coursera.org/learn/ml-clustering-and-retrieval)

<table>
  <tr>
    <th>Description</th>
    <th>Programming Assignments</th>
  </tr>
  <tr>
    <td>
      <table>
        <tr>
          <td>Models</td>
          <td><ul><li>Nearest neighbors</li><li>Clustering, mixtures of Gaussians</li><li>Latent Dirichlet allocation (LDA)</li></ul></td>
        </tr>
        <tr>
          <td>Algorithms</td>
          <td><ul><li>K-means, MapReduce</li><li>K-NN, KD-trees, locality-sensitive hashing (LSH)</li><li>Expectation-maximization (EM)</li><li>Gibbs sampling</li></ul></td>
        </tr>
        <tr>
          <td>Concepts</td>
          <td><ul><li>Distance metrics, approximation algorithms,</li><li>hashing, sampling algorithms, scaling up with map-reduce</li></ul></td>
        </tr>
        <tr>
          <td>Core ML</td>
          <td><ul><li>Unsupervised learning</li><li>Probabilistic modeling</li><li>Data parallel problems</li><li>Bayesian inference</li></ul></td>
        </tr>
      </table>
    </td>
    <td>
      <ul>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%201%20PA%201">Choosing features and metrics for nearest neighbor search</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%201%20PA%202">Implementing Locality Sensitive Hashing from scratch</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%203%20PA%201">Clustering text data with k-means</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%204%20PA%201">Implementing EM for Gaussian mixtures</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%204%20PA%202">Clustering text data with Gaussian mixtures</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%205%20PA%201">Modeling text topics with Latent Dirichlet Allocation</a>
        </li>
        <li>
          [x] <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%206%20PA%201">Modeling text data with a hierarchy of clusters</a>
      </ul>
    </td>
  </tr>
</table>

Slides and more details about this course can be found in my Github [SSQ](https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval)

- Week 1 Intro

- Week 2 Nearest Neighbor Search: Retrieving Documents
  - Implement nearest neighbor search for retrieval tasks
  - Contrast document representations (e.g., raw word counts, tf-idf,…)
    - Emphasize important words using tf-idf
  - Contrast methods for measuring similarity between two documents
    - Euclidean vs. weighted Euclidean
    - Cosine similarity vs. similarity via unnormalized inner product
  - Describe complexity of brute force search
  - Implement KD-trees for nearest neighbor search
  - Implement LSH for approximate nearest neighbor search
  - Compare pros and cons of KD-trees and LSH, and decide which is more appropriate for given dataset
  - [x] [Choosing features and metrics for nearest neighbor search](https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%201%20PA%201)
  - [x] [Implementing Locality Sensitive Hashing from scratch](https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%201%20PA%202)

- Week 3 Clustering with k-means
  - Describe potential applications of clustering
  - Describe the input (unlabeled observations) and output (labels) of a clustering algorithm
  - Determine whether a task is supervised or unsupervised
  - Cluster documents using k-means
  - Interpret k-means as a coordinate descent algorithm
  - Define data parallel problems
  - Explain Map and Reduce steps of MapReduce framework
  - Use existing MapReduce implementations to parallelize kmeans, understanding what’s being done under the hood
  - [x] [Clustering text data with k-means](https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%203%20PA%201)
  
- Week 4 Mixture Models: Model-Based Clustering
  - Interpret a probabilistic model-based approach to clustering using mixture models
  - Describe model parameters
  - Motivate the utility of soft assignments and describe what they represent
  - Discuss issues related to how the number of parameters grow with the number of dimensions
    - Interpret diagonal covariance versions of mixtures of Gaussians
  - Compare and contrast mixtures of Gaussians and k-means
  - Implement an EM algorithm for inferring soft assignments and cluster parameters
    - Determine an initialization strategy
    - Implement a variant that helps avoid overfitting issues
  - [x] [Implementing EM for Gaussian mixtures](https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%204%20PA%201)
  - [x] [Clustering text data with Gaussian mixtures](https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%204%20PA%202)
    
- Week 5 Latent Dirichlet Allocation: Mixed Membership Modeling
  - Compare and contrast clustering and mixed membership models
  - Describe a document clustering model for the bagof-words doc representation
  - Interpret the components of the LDA mixed membership model
  - Analyze a learned LDA model
    - Topics in the corpus
    - Topics per document
  - Describe Gibbs sampling steps at a high level
  - Utilize Gibbs sampling output to form predictions or estimate model parameters
  - Implement collapsed Gibbs sampling for LDA
  - [x] [Modeling text topics with Latent Dirichlet Allocation](https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%205%20PA%201)
  
- Week 6 Hierarchical Clustering & Closing Remarks
  - Bonus content: Hierarchical clustering
    - Divisive clustering
    - Agglomerative clustering
      - The dendrogram for agglomerative clustering
      - Agglomerative clustering details
  - Hidden Markov models (HMMs): Another notion of “clustering”
  - [x] [Modeling text data with a hierarchy of clusters](https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Week%206%20PA%201)
